{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as  pd \n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../cna_data/train_pub.json\")as f:\n",
    "    public = json.load(f)\n",
    "with open(\"../cna_data/train_author.json\")as f:\n",
    "    Author = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建训练集\n",
    "- 选取适合的作者及其论文\n",
    "- 针对某一作者、某篇论文生成特征\n",
    "- 针对同名作者、同篇论文生成特征\n",
    "- 特征是由作者+论文共同生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negNum = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205416\n"
     ]
    }
   ],
   "source": [
    "# 选取同名人数大于指定值的作者作为训练集\n",
    "paperAndAuthor = []\n",
    "authorPaper = {}\n",
    "for name in Author:\n",
    "#     print(len(name))\n",
    "    if len(Author[name].keys()) > negNum:\n",
    "        for person in Author[name]:\n",
    "            authorPaper[person] = Author[name][person]\n",
    "            for paper in Author[name][person]:\n",
    "                temp = paper + '-' + person + '-' + name\n",
    "#                 print(temp)\n",
    "                paperAndAuthor.append(temp)\n",
    "print(len(paperAndAuthor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查看pub文件中论文作者姓名书写方式\n",
    "- 连字符\n",
    "- encode('utf-8') 接 decode('utf-8')\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zuozhe = set()\n",
    "for artical in public:\n",
    "#     print(artical)\n",
    "#     authors = artical[\"authors\"]\n",
    "    for author in public[artical][\"authors\"]:\n",
    "#         print(author)\n",
    "        temp = author[\"name\"].encode('utf-8')\n",
    "        item = temp.decode('utf-8')\n",
    "        if item not in zuozhe:\n",
    "            zuozhe.add(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整理作者姓名\n",
    "- 对于中文姓名使用 名_姓 表示\n",
    "- 对于全空格分隔的名字，使用连字符将其连接\n",
    "- 对于包含.和空格的名字，将. 和空格替换为_\n",
    "- 对于中文名字 姓在前名在后的，将两者位置交换\n",
    "- 还存在的问题：对于中文名字姓在前名在后的情况若名字中无连字符则无法将姓名颠倒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cleanName(dirtyName):\n",
    "    \n",
    "    name = dirtyName.lower()\n",
    "    name = name.replace('\\xa0', ' ')\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace('dr.', '')\n",
    "    name = name.replace('dr ', '')\n",
    "    name = name.replace(' 0001', '')\n",
    "    temp = name.split(' ')\n",
    "    if len(temp) == 2:\n",
    "        if '-' in temp[1] and '-' not in temp[0]:\n",
    "            a = temp[1]\n",
    "            temp[1] = temp[0]\n",
    "            temp[0] = a\n",
    "    k = []\n",
    "    for t in temp:\n",
    "        if t != '' and t != ' ':\n",
    "            k.append(t)\n",
    "    name = '_'.join(k)\n",
    "    name = name.replace('-', '')\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getSample(item):\n",
    "    sample = []\n",
    "    paper, author, name = item.split('-')\n",
    "    sample.append('-'.join([paper, author, name, '1']))\n",
    "\n",
    "    candidateList = list(Author[name].keys())\n",
    "    candidateList.remove(author)\n",
    "    negPeople = random.sample(candidateList, negNum)\n",
    "    for negPerson in negPeople:\n",
    "        sample.append('-'.join([paper, negPerson, name, '0']))\n",
    "    return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def entropy(dic):\n",
    "    count = []\n",
    "    total = 0\n",
    "    ans = 0\n",
    "    for item in dic:\n",
    "        count.append(dic[item])\n",
    "        total += dic[item]\n",
    "    prob = [item/total for item in count]\n",
    "    for item in prob:\n",
    "        ans += -1 * item * round(np.log(item), 6)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def cleanYear(dic):\n",
    "    count = []\n",
    "    largest = 0\n",
    "    smallest = 3000\n",
    "    err = 0\n",
    "    ans = defaultdict(int)\n",
    "    for item in dic:\n",
    "        if item == '' or item > 2019 or item < 1950 :\n",
    "            err += dic[item]\n",
    "        else:\n",
    "            count.append(dic[item])\n",
    "            ans[item] = dic[item]\n",
    "            if item > largest:\n",
    "                largest = item\n",
    "            if item < smallest:\n",
    "                smallest = item\n",
    "    mean = np.mean(count)\n",
    "    if smallest < largest:\n",
    "        add = np.random.randint(smallest, largest, err)\n",
    "    else:\n",
    "        add = [smallest] * err\n",
    "    for data in add:\n",
    "        ans[data] += 1\n",
    "    count = []\n",
    "    for item in ans:\n",
    "        count.append(ans[item])\n",
    "    mean = np.mean(count)\n",
    "    for item in ans:\n",
    "        ans[item] = 1/(1 + np.abs(mean - ans[item]))\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getFeature(item):\n",
    "    stopWord = ['', 'a', 'an', ':', 'the', '.', ',', '\\n', '(', ')', ]\n",
    "    paperId, authorId, _, label = item.split('-')\n",
    "    \n",
    "    paperInfo = public[paperId]\n",
    "    \n",
    "    title = paperInfo[\"title\"]\n",
    "    authors = paperInfo[\"authors\"]\n",
    "    venue = paperInfo[\"venue\"]\n",
    "    year = paperInfo.get(\"year\", 0)\n",
    "    keyword = paperInfo.get(\"keywords\", \"\")\n",
    "    if keyword == None:\n",
    "        keyword = [\"\"]\n",
    "    abstract = paperInfo.get(\"abstract\", \"\")\n",
    "    if abstract == None:\n",
    "        abstract = \"\"\n",
    "    #这里不对待查询作者进行删除\n",
    "    #对待查询论文进行预处理，获取其相关特征\n",
    "    #特征包括titleLength、words、coauthors、coauthorsLength、coauthorsDiffLength、\n",
    "    #        orgs、orgsDiffLength、venu、year、keywords、abstractLength\n",
    "    #题目长度、题目词频\n",
    "    titles = title.split()\n",
    "    titleLength = 0\n",
    "    words = defaultdict(int)\n",
    "    for item in titles:\n",
    "        if item.lower() not in stopWord:\n",
    "            titleLength += 1\n",
    "            words[item.lower()] += 1\n",
    "            \n",
    "    #合作者个数、频率\n",
    "    coauthors = defaultdict(int)\n",
    "    coauthorsLength = 0\n",
    "    for item in authors:\n",
    "        if item[\"name\"] != '':\n",
    "            coauthors[cleanName(item[\"name\"])] += 1\n",
    "            coauthorsLength += 1\n",
    "    coauthorsDiffLength = len(coauthors)\n",
    "    \n",
    "    #合作机构频率\n",
    "    orgs = defaultdict(int)\n",
    "    for item in authors:\n",
    "        orgs[item.get(\"org\", \"\").lower()] += 1\n",
    "    orgsDiffLength = len(orgs)\n",
    "            \n",
    "    #发表年份可能不存在\n",
    "            \n",
    "    #关键词词频\n",
    "    keywords = defaultdict(int)\n",
    "    for item in keyword:\n",
    "        if item != '':\n",
    "            keywords[item.lower()] += 1\n",
    "    \n",
    "    #摘要长度、词频\n",
    "    abstractLength = 0\n",
    "    abstracts = abstract.split()\n",
    "    for item in abstracts:\n",
    "        if item not in stopWord:\n",
    "            abstractLength += 1\n",
    "            words[item.lower()] += 1\n",
    "    \n",
    "    \n",
    "    relatedTitleLength = 0               #题目平均长度 1\n",
    "    relatedCoauthorLength = 0            #合作者数量1\n",
    "    relatedCoauthorDiffLength = 0        #合作者人数1\n",
    "    relatedOrgsDiffLength = 0            #合作机构个数1\n",
    "    relatedAbstractLength = 0            #摘要长度\n",
    "    relatedCoauthors = defaultdict(int)  #合作者1\n",
    "    relatedOrgs = defaultdict(int)       #合作机构1\n",
    "    relatedVenue = defaultdict(int)      #发表期刊1\n",
    "    relatedYear = defaultdict(int)       #发表年份1\n",
    "    relatedKeyword = defaultdict(int)    #关键词1\n",
    "    relatedWords = defaultdict(int)      #题目、摘要出现词\n",
    "    relatedOrgEntropy = 0\n",
    "    \n",
    "    relatedPapers = authorPaper[authorId]\n",
    "    #去除待分类论文\n",
    "    if paperId in relatedPapers:\n",
    "        relatedPapers.remove(paperId)\n",
    "    for relatedPaper in relatedPapers:\n",
    "        relatedPaperInfo = public[relatedPaper]\n",
    "        \n",
    "        tempAuthors = relatedPaperInfo[\"authors\"]\n",
    "        tempOrgs = defaultdict(int)\n",
    "        for tempName in tempAuthors:\n",
    "            #机构、合作者频率\n",
    "            #合作者数量\n",
    "            relatedOrgs[tempName.get(\"org\", \"\")] += 1\n",
    "            relatedCoauthors[cleanName(tempName[\"name\"])] += 1\n",
    "            tempOrgs[tempName.get(\"org\", \"\")] += 1\n",
    "            relatedCoauthorLength += 1\n",
    "        #机构平均散度\n",
    "        relatedOrgEntropy += entropy(tempOrgs)\n",
    "        #年份\n",
    "        relatedYear[relatedPaperInfo.get(\"year\", 0)] += 1\n",
    "        #期刊\n",
    "        relatedVenue[relatedPaperInfo.get(\"venue\", \"none\")] += 1\n",
    "        #关键词\n",
    "        for item in relatedPaperInfo.get(\"keywords\", \"\"):\n",
    "            if item != '':\n",
    "                relatedKeyword[item.lower()] += 1\n",
    "        #题目长度\n",
    "        tempTitle = relatedPaperInfo[\"title\"].split()\n",
    "        for item in tempTitle:\n",
    "            if item not in stopWord:\n",
    "                relatedTitleLength += 1\n",
    "                relatedWords[item.lower()] += 1\n",
    "        #摘要\n",
    "        tempAbstracts = relatedPaperInfo.get(\"abstract\", \" \")\n",
    "        if tempAbstracts == None:\n",
    "            tempAbstracts = \"\"\n",
    "        tempAbstracts = tempAbstracts.split()\n",
    "        \n",
    "        for item in tempAbstracts:\n",
    "            if item not in stopWord:\n",
    "                relatedAbstractLength += 1\n",
    "                relatedWords[item.lower()] += 1\n",
    "        \n",
    "    relatedTitleLengthAve = relatedTitleLength/(len(relatedPapers) + 1)\n",
    "    relatedCoauthorLengthAve = relatedCoauthorLength/(len(relatedPapers) + 1)\n",
    "    relatedAbstractLengthAve = relatedAbstractLength/(len(relatedPapers) + 1)\n",
    "    relatedCoauthorDiffLength = len(relatedCoauthors)\n",
    "    relatedOrgsDiffLength = len(relatedOrgs)\n",
    "    relatedOrgsDiffLengthAve = len(relatedOrgs)/(len(relatedPapers) + 1)\n",
    "    relatedOrgEntropyAve = relatedOrgEntropy/(len(relatedPapers) + 1)\n",
    "    #根据待查询论文与待查询作者的相关信息生成特征\n",
    "    #1、题目长度 待查询论文题目长度/待查询作者论文题目平均长度\n",
    "    scoreTitle = titleLength/(relatedTitleLengthAve + 1)\n",
    "    \n",
    "    #2、合作者相关\n",
    "    # -1重合的合作者数\n",
    "    # -2待查询论文的合作者数/待查询作者的平均合作者数(relatedCoauthorLengthAve)\n",
    "    # -3重合的合作者数/待查询论文的合作者数(coauthorsLength)\n",
    "    # -4重合的合作者数/待查询作者的合作者数(relatedCoauthorDiffLength)\n",
    "    # -5重合的合作者的总合作次数\n",
    "    # -6重合的合作者的总合作次数/所有合作者的总合作次数\n",
    "    \n",
    "    relatedCoauthorsSet = set(relatedCoauthors.keys())\n",
    "    coauthorsSet = set(coauthors.keys())\n",
    "    scoreAuthor1 = len(relatedCoauthorsSet & coauthorsSet)\n",
    "    scoreAuthor2 = coauthorsLength /(relatedCoauthorLengthAve + 1)\n",
    "    scoreAuthor3 = scoreAuthor1 /(coauthorsLength + 1)\n",
    "    scoreAuthor4 = scoreAuthor1 /(relatedCoauthorDiffLength + 1)\n",
    "    scoreAuthor5 = 0\n",
    "    for item in coauthorsSet:\n",
    "        scoreAuthor5 += relatedCoauthors[item]\n",
    "    scoreAuthor6 = scoreAuthor5/(relatedCoauthorLength + 1)\n",
    "    \n",
    "    #3、期刊相关\n",
    "    # -1 重合期刊的发表次数\n",
    "    # -2 重合期刊的发表次数/总发表期刊次数\n",
    "    \n",
    "    scoreVenue1 = relatedVenue.get(venue, 0)\n",
    "    temp = 0\n",
    "    for item in relatedVenue:\n",
    "        temp += relatedVenue[item]\n",
    "    scoreVenue2 = scoreVenue1/(temp + 1)\n",
    "    \n",
    "    #4、合作机构相关\n",
    "    # -1 待查询论文合作机构(orgsDiffLength)\n",
    "    # -2 待查询论文合作机构/待查询作者平均合作机构\n",
    "    # -3 待查询论文合作机构总合作次数\n",
    "    # -4 待查询论文合作机构总合作次数/待查询作者所有合作机构总合作次数\n",
    "    # -5 待查询论文合作机构散度\n",
    "    # -6 待查询论文合作机构散度/待查询作者所有合作机构平均散度\n",
    "    scoreOrg1 = orgsDiffLength\n",
    "    scoreOrg2 = orgsDiffLength/(relatedOrgsDiffLengthAve + 1)\n",
    "    scoreOrg3 = 0\n",
    "    for item in orgs:\n",
    "        scoreOrg3 += relatedOrgs[item]\n",
    "    temp = 0\n",
    "    for item in relatedOrgs:\n",
    "        temp += relatedOrgs[item]\n",
    "    scoreOrg4 = scoreOrg3/(temp + 1)\n",
    "    scoreOrg5 = entropy(orgs)\n",
    "    scoreOrg6 = scoreOrg5/(relatedOrgEntropyAve + 1)\n",
    "    \n",
    "    #5、发表时间\n",
    "    #我们认为同一作者不同年份发表数量均匀\n",
    "    #则对偏离平均发表数量的年份做惩罚\n",
    "    \n",
    "    if year == 0:\n",
    "        scoreYear = 0\n",
    "    else:\n",
    "        scoreYear = cleanYear(relatedYear)[year]\n",
    "        \n",
    "    #6、关键词相关\n",
    "    # -1 重合关键词总次数\n",
    "    # -2 总关键词次数\n",
    "    \n",
    "    originKeywords = set(keywords.keys())\n",
    "    relatedKeywords = set(relatedKeyword.keys())\n",
    "    scoreKeyword1 = len(originKeywords & relatedKeywords)\n",
    "    scoreKeyword2 = 0\n",
    "    for item in relatedKeyword:\n",
    "        scoreKeyword2 += relatedKeyword[item]\n",
    "    \n",
    "    \n",
    "    #7、摘要相关\n",
    "    # -1 待查询论文摘要长度/平均摘要长度\n",
    "    # -2 待查询论文摘要关键词重合个数\n",
    "    # -3 关键词重合总次数\n",
    "    # -4 关键词重合总次数/总关键词出现次数\n",
    "    \n",
    "    scoreAbstract1 = abstractLength/(relatedAbstractLengthAve + 1)\n",
    "    originWord = set(words.keys())\n",
    "    relatedWord = set(relatedWords.keys())\n",
    "    scoreAbstract2 = len(originWord & relatedWord)\n",
    "    scoreAbstract3 = 0\n",
    "    for item in words:\n",
    "        scoreAbstract3 += relatedWords.get(item, 0)\n",
    "    temp = 0\n",
    "    for item in relatedWords:\n",
    "        temp += relatedWords[item]\n",
    "    scoreAbstract4 = scoreAbstract3/(temp + 1)\n",
    "    \n",
    "    # 所有特征\n",
    "    # 1、scoreTitle\n",
    "    # 2、scoreAuthor1\n",
    "    # 3、scoreAuthor2 \n",
    "    # 4、scoreAuthor3\n",
    "    # 5、scoreAuthor4\n",
    "    # 6、scoreAuthor5\n",
    "    # 7、scoreAuthor6\n",
    "    # 8、scoreVenue1\n",
    "    # 9、scoreVenue2 \n",
    "    # 10、scoreOrg1\n",
    "    # 11、scoreOrg2\n",
    "    # 12、scoreOrg3 \n",
    "    # 13、scoreOrg4\n",
    "    # 14、scoreOrg5\n",
    "    # 15、scoreOrg6 \n",
    "    # 16、scoreYear\n",
    "    # 17、scoreKeyword1\n",
    "    # 18、scoreKeyword2 \n",
    "    # 19、scoreAbstract1\n",
    "    # 10、scoreAbstract2\n",
    "    # 21、scoreAbstract3 \n",
    "    # 21、scoreAbstract4\n",
    "    \n",
    "    feature = [round(scoreTitle, 6),\n",
    "               round(scoreAuthor1, 6),\n",
    "               round(scoreAuthor2, 6),\n",
    "               round(scoreAuthor3, 6),\n",
    "               round(scoreAuthor4, 6),\n",
    "               round(scoreAuthor5, 6),\n",
    "               round(scoreAuthor6, 6),\n",
    "               round(scoreVenue1, 6),\n",
    "               round(scoreVenue2, 6),\n",
    "               round(scoreOrg1, 6),\n",
    "               round(scoreOrg2, 6),\n",
    "               round(scoreOrg3, 6),\n",
    "               round(scoreOrg4, 6),\n",
    "               round(scoreOrg5, 6),\n",
    "               round(scoreOrg6, 6),\n",
    "               round(scoreYear, 6),\n",
    "               round(scoreKeyword1, 6),\n",
    "               round(scoreKeyword2, 6),\n",
    "               round(scoreAbstract1, 6),\n",
    "               round(scoreAbstract2, 6),\n",
    "               round(scoreAbstract3, 6),\n",
    "               round(scoreAbstract4, 6)\n",
    "              ]\n",
    "    return [feature, label]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vwt9Wd81', 'MLEuzS1T', 'xOEKHNxv', 'Ms4yXx88', 'VosVgGWX', 'H7pPNJ9H', 'Izy4g5GJ', 'KCYgj45o', 'QgxWVd6S', '7mR9sXDN', '3qn5CxG9']\n"
     ]
    }
   ],
   "source": [
    "a = list(Author[name].keys())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/616248 [00:00<41:17, 248.75it/s]/home/zuijiang/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/zuijiang/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 616248/616248 [38:12<00:00, 268.83it/s]  \n"
     ]
    }
   ],
   "source": [
    "pickleText = []\n",
    "trainSet = paperAndAuthor\n",
    "posAndNegSample = []\n",
    "random.shuffle(trainSet)\n",
    "for item in trainSet:\n",
    "    posAndNegSample.extend(getSample(item))\n",
    "\n",
    "result = []\n",
    "for item in tqdm(posAndNegSample):\n",
    "    result.append(getFeature(item))\n",
    "pickleText.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616248/616248 [00:00<00:00, 1802247.75it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = []\n",
    "for item in tqdm(pickleText):\n",
    "    f, l = item\n",
    "    l = int(l)\n",
    "    f.append(l)\n",
    "    sample.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616248, 23)\n"
     ]
    }
   ],
   "source": [
    "test = np.array(sample)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(test)\n",
    "features = DataFrame(test, columns=[\"Title\", \"Author1\", \"Author2\", \"Author3\",\n",
    "                                   \"Author4\", \"Author5\", \"Author6\", \"Venue1\",\n",
    "                                   \"Venue2\", \"Org1\", \"Org2\", \"Org3\",\n",
    "                                   \"Org4\", \"Org5\", \"Org6\", \"Year\",\n",
    "                                   \"Keyword1\", \"Keyword2\", \"Abstract1\", \"Abstract2\",\n",
    "                                   \"Abstract3\", \"Abstract4\",\n",
    "                                   \"label\",\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFeature = (features - features.mean())/features.std()\n",
    "labelSample = features[\"label\"]\n",
    "col = [item for item in features.columns if item != \"label\"]\n",
    "featureSample = features[col]\n",
    "tempFeature = (featureSample - featureSample.mean())/featureSample.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616248, 22)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tempFeature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title           1.923687\n",
       "Author1         8.532953\n",
       "Author2         6.122245\n",
       "Author3         0.282686\n",
       "Author4         0.091565\n",
       "Author5       863.847628\n",
       "Author6         0.195122\n",
       "Venue1          1.611264\n",
       "Venue2          0.030789\n",
       "Org1            2.996878\n",
       "Org2            1.536680\n",
       "Org3          576.058392\n",
       "Org4            0.092138\n",
       "Org5            0.391260\n",
       "Org6            0.305145\n",
       "Year            0.214444\n",
       "Keyword1        0.364196\n",
       "Keyword2      177.177828\n",
       "Abstract1      15.909005\n",
       "Abstract2      18.852115\n",
       "Abstract3    1041.499956\n",
       "Abstract4       0.168549\n",
       "label           0.333707\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../cna_data/whole_author_profile_pub.json\")as f:\n",
    "    wholePublic = json.load(f)\n",
    "with open(\"../cna_data/cna_test_pub.json\")as f:\n",
    "    testPublic = json.load(f)\n",
    "with open(\"../cna_data/whole_author_profile.json\")as f:\n",
    "    authorPaper = json.load(f)\n",
    "with open(\"../cna_data/cna_test_unass_competition.json\")as f:\n",
    "    testData = json.load(f)\n",
    "    \n",
    "label = np.array(labelSample)\n",
    "feature = np.array(tempFeature)\n",
    "means = np.array(features.mean().tolist()[:-1])\n",
    "stds = np.array(features.std().tolist()[:-1])\n",
    "features = feature\n",
    "labels = label\n",
    "mean = means\n",
    "std = stds\n",
    "\n",
    "xSample = features.tolist()\n",
    "ySample = labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     8,
     274
    ]
   },
   "outputs": [],
   "source": [
    "trainNum = 500000\n",
    "xTrain = xSample[:trainNum]\n",
    "yTrain = ySample[:trainNum]\n",
    "xTest = xSample[trainNum:]\n",
    "yTest = ySample[trainNum:]\n",
    "dtrain = xgb.DMatrix(xTrain, label=yTrain)\n",
    "dtest = xgb.DMatrix(xTest, label=yTest)\n",
    "\n",
    "def getFeature(item):\n",
    "    stopWord = ['', 'a', 'an', ':', 'the', '.', ',', '\\n', '(', ')', ]\n",
    "    paperId, authorId = item.split('-')\n",
    "    \n",
    "    paperInfo = testPublic[paperId]\n",
    "    \n",
    "    title = paperInfo[\"title\"]\n",
    "    authors = paperInfo[\"authors\"]\n",
    "    venue = paperInfo[\"venue\"]\n",
    "    year = paperInfo.get(\"year\", 0)\n",
    "    keyword = paperInfo.get(\"keywords\", \"\")\n",
    "    if keyword == None:\n",
    "        keyword = [\"\"]\n",
    "    abstract = paperInfo.get(\"abstract\", \"\")\n",
    "    if abstract == None:\n",
    "        abstract = \"\"\n",
    "    #这里不对待查询作者进行删除\n",
    "    #对待查询论文进行预处理，获取其相关特征\n",
    "    #特征包括titleLength、words、coauthors、coauthorsLength、coauthorsDiffLength、\n",
    "    #        orgs、orgsDiffLength、venu、year、keywords、abstractLength\n",
    "    #题目长度、题目词频\n",
    "    titles = title.split()\n",
    "    titleLength = 0\n",
    "    words = defaultdict(int)\n",
    "    for item in titles:\n",
    "        if item.lower() not in stopWord:\n",
    "            titleLength += 1\n",
    "            words[item.lower()] += 1\n",
    "            \n",
    "    #合作者个数、频率\n",
    "    coauthors = defaultdict(int)\n",
    "    coauthorsLength = 0\n",
    "    for item in authors:\n",
    "        if item[\"name\"] != '':\n",
    "            coauthors[cleanName(item[\"name\"])] += 1\n",
    "            coauthorsLength += 1\n",
    "    coauthorsDiffLength = len(coauthors)\n",
    "    \n",
    "    #合作机构频率\n",
    "    orgs = defaultdict(int)\n",
    "    for item in authors:\n",
    "        orgs[item.get(\"org\", \"\").lower()] += 1\n",
    "    orgsDiffLength = len(orgs)\n",
    "            \n",
    "    #发表年份可能不存在\n",
    "            \n",
    "    #关键词词频\n",
    "    keywords = defaultdict(int)\n",
    "    for item in keyword:\n",
    "        if item != '':\n",
    "            keywords[item.lower()] += 1\n",
    "    \n",
    "    #摘要长度、词频\n",
    "    abstractLength = 0\n",
    "    abstracts = abstract.split()\n",
    "    for item in abstracts:\n",
    "        if item not in stopWord:\n",
    "            abstractLength += 1\n",
    "            words[item.lower()] += 1\n",
    "    \n",
    "    \n",
    "    relatedTitleLength = 0               #题目平均长度 1\n",
    "    relatedCoauthorLength = 0            #合作者数量1\n",
    "    relatedCoauthorDiffLength = 0        #合作者人数1\n",
    "    relatedOrgsDiffLength = 0            #合作机构个数1\n",
    "    relatedAbstractLength = 0            #摘要长度\n",
    "    relatedCoauthors = defaultdict(int)  #合作者1\n",
    "    relatedOrgs = defaultdict(int)       #合作机构1\n",
    "    relatedVenue = defaultdict(int)      #发表期刊1\n",
    "    relatedYear = defaultdict(int)       #发表年份1\n",
    "    relatedKeyword = defaultdict(int)    #关键词1\n",
    "    relatedWords = defaultdict(int)      #题目、摘要出现词\n",
    "    relatedOrgEntropy = 0\n",
    "    \n",
    "    relatedPapers = authorPaper[authorId][\"papers\"]\n",
    "    #去除待分类论文\n",
    "    if paperId in relatedPapers:\n",
    "        relatedPapers.remove(paperId)\n",
    "    for relatedPaper in relatedPapers:\n",
    "#         print(relatedPaper)\n",
    "        relatedPaperInfo = wholePublic[relatedPaper]\n",
    "        \n",
    "        tempAuthors = relatedPaperInfo[\"authors\"]\n",
    "        tempOrgs = defaultdict(int)\n",
    "        for tempName in tempAuthors:\n",
    "            #机构、合作者频率\n",
    "            #合作者数量\n",
    "            relatedOrgs[tempName.get(\"org\", \"\")] += 1\n",
    "            relatedCoauthors[cleanName(tempName[\"name\"])] += 1\n",
    "            tempOrgs[tempName.get(\"org\", \"\")] += 1\n",
    "            relatedCoauthorLength += 1\n",
    "        #机构平均散度\n",
    "        relatedOrgEntropy += entropy(tempOrgs)\n",
    "        #年份\n",
    "        relatedYear[relatedPaperInfo.get(\"year\", 0)] += 1\n",
    "        #期刊\n",
    "        relatedVenue[relatedPaperInfo.get(\"venue\", \"none\")] += 1\n",
    "        #关键词\n",
    "        for item in relatedPaperInfo.get(\"keywords\", \"\"):\n",
    "            if item != '':\n",
    "                relatedKeyword[item.lower()] += 1\n",
    "        #题目长度\n",
    "        tempTitle = relatedPaperInfo[\"title\"].split()\n",
    "        for item in tempTitle:\n",
    "            if item not in stopWord:\n",
    "                relatedTitleLength += 1\n",
    "                relatedWords[item.lower()] += 1\n",
    "        #摘要\n",
    "        tempAbstracts = relatedPaperInfo.get(\"abstract\", \" \")\n",
    "        if tempAbstracts == None:\n",
    "            tempAbstracts = \"\"\n",
    "        tempAbstracts = tempAbstracts.split()\n",
    "        \n",
    "        for item in tempAbstracts:\n",
    "            if item not in stopWord:\n",
    "                relatedAbstractLength += 1\n",
    "                relatedWords[item.lower()] += 1\n",
    "        \n",
    "    relatedTitleLengthAve = relatedTitleLength/(len(relatedPapers) + 1)\n",
    "    relatedCoauthorLengthAve = relatedCoauthorLength/(len(relatedPapers) + 1)\n",
    "    relatedAbstractLengthAve = relatedAbstractLength/(len(relatedPapers) + 1)\n",
    "    relatedCoauthorDiffLength = len(relatedCoauthors)\n",
    "    relatedOrgsDiffLength = len(relatedOrgs)\n",
    "    relatedOrgsDiffLengthAve = len(relatedOrgs)/(len(relatedPapers) + 1)\n",
    "    relatedOrgEntropyAve = relatedOrgEntropy/(len(relatedPapers) + 1)\n",
    "    #根据待查询论文与待查询作者的相关信息生成特征\n",
    "    #1、题目长度 待查询论文题目长度/待查询作者论文题目平均长度\n",
    "    scoreTitle = titleLength/(relatedTitleLengthAve + 1)\n",
    "    \n",
    "    #2、合作者相关\n",
    "    # -1重合的合作者数\n",
    "    # -2待查询论文的合作者数/待查询作者的平均合作者数(relatedCoauthorLengthAve)\n",
    "    # -3重合的合作者数/待查询论文的合作者数(coauthorsLength)\n",
    "    # -4重合的合作者数/待查询作者的合作者数(relatedCoauthorDiffLength)\n",
    "    # -5重合的合作者的总合作次数\n",
    "    # -6重合的合作者的总合作次数/所有合作者的总合作次数\n",
    "    \n",
    "    relatedCoauthorsSet = set(relatedCoauthors.keys())\n",
    "    coauthorsSet = set(coauthors.keys())\n",
    "    scoreAuthor1 = len(relatedCoauthorsSet & coauthorsSet)\n",
    "    scoreAuthor2 = coauthorsLength /(relatedCoauthorLengthAve + 1)\n",
    "    scoreAuthor3 = scoreAuthor1 /(coauthorsLength + 1)\n",
    "    scoreAuthor4 = scoreAuthor1 /(relatedCoauthorDiffLength + 1)\n",
    "    scoreAuthor5 = 0\n",
    "    for item in coauthorsSet:\n",
    "        scoreAuthor5 += relatedCoauthors[item]\n",
    "    scoreAuthor6 = scoreAuthor5/(relatedCoauthorLength + 1)\n",
    "    \n",
    "    #3、期刊相关\n",
    "    # -1 重合期刊的发表次数\n",
    "    # -2 重合期刊的发表次数/总发表期刊次数\n",
    "    \n",
    "    scoreVenue1 = relatedVenue.get(venue, 0)\n",
    "    temp = 0\n",
    "    for item in relatedVenue:\n",
    "        temp += relatedVenue[item]\n",
    "    scoreVenue2 = scoreVenue1/(temp + 1)\n",
    "    \n",
    "    #4、合作机构相关\n",
    "    # -1 待查询论文合作机构(orgsDiffLength)\n",
    "    # -2 待查询论文合作机构/待查询作者平均合作机构\n",
    "    # -3 待查询论文合作机构总合作次数\n",
    "    # -4 待查询论文合作机构总合作次数/待查询作者所有合作机构总合作次数\n",
    "    # -5 待查询论文合作机构散度\n",
    "    # -6 待查询论文合作机构散度/待查询作者所有合作机构平均散度\n",
    "    scoreOrg1 = orgsDiffLength\n",
    "    scoreOrg2 = orgsDiffLength/(relatedOrgsDiffLengthAve + 1)\n",
    "    scoreOrg3 = 0\n",
    "    for item in orgs:\n",
    "        scoreOrg3 += relatedOrgs[item]\n",
    "    temp = 0\n",
    "    for item in relatedOrgs:\n",
    "        temp += relatedOrgs[item]\n",
    "    scoreOrg4 = scoreOrg3/(temp + 1)\n",
    "    scoreOrg5 = entropy(orgs)\n",
    "    scoreOrg6 = scoreOrg5/(relatedOrgEntropyAve + 1)\n",
    "    \n",
    "    #5、发表时间\n",
    "    #我们认为同一作者不同年份发表数量均匀\n",
    "    #则对偏离平均发表数量的年份做惩罚\n",
    "    \n",
    "    if year == 0:\n",
    "        scoreYear = 0\n",
    "    else:\n",
    "        scoreYear = cleanYear(relatedYear)[year]\n",
    "        \n",
    "    #6、关键词相关\n",
    "    # -1 重合关键词总次数\n",
    "    # -2 总关键词次数\n",
    "    \n",
    "    originKeywords = set(keywords.keys())\n",
    "    relatedKeywords = set(relatedKeyword.keys())\n",
    "    scoreKeyword1 = len(originKeywords & relatedKeywords)\n",
    "    scoreKeyword2 = 0\n",
    "    for item in relatedKeyword:\n",
    "        scoreKeyword2 += relatedKeyword[item]\n",
    "    \n",
    "    \n",
    "    #7、摘要相关\n",
    "    # -1 待查询论文摘要长度/平均摘要长度\n",
    "    # -2 待查询论文摘要关键词重合个数\n",
    "    # -3 关键词重合总次数\n",
    "    # -4 关键词重合总次数/总关键词出现次数\n",
    "    \n",
    "    scoreAbstract1 = abstractLength/(relatedAbstractLengthAve + 1)\n",
    "    originWord = set(words.keys())\n",
    "    relatedWord = set(relatedWords.keys())\n",
    "    scoreAbstract2 = len(originWord & relatedWord)\n",
    "    scoreAbstract3 = 0\n",
    "    for item in words:\n",
    "        scoreAbstract3 += relatedWords.get(item, 0)\n",
    "    temp = 0\n",
    "    for item in relatedWords:\n",
    "        temp += relatedWords[item]\n",
    "    scoreAbstract4 = scoreAbstract3/(temp + 1)\n",
    "    \n",
    "    # 所有特征\n",
    "    # 1、scoreTitle\n",
    "    # 2、scoreAuthor1\n",
    "    # 3、scoreAuthor2 \n",
    "    # 4、scoreAuthor3\n",
    "    # 5、scoreAuthor4\n",
    "    # 6、scoreAuthor5\n",
    "    # 7、scoreAuthor6\n",
    "    # 8、scoreVenue1\n",
    "    # 9、scoreVenue2 \n",
    "    # 10、scoreOrg1\n",
    "    # 11、scoreOrg2\n",
    "    # 12、scoreOrg3 \n",
    "    # 13、scoreOrg4\n",
    "    # 14、scoreOrg5\n",
    "    # 15、scoreOrg6 \n",
    "    # 16、scoreYear\n",
    "    # 17、scoreKeyword1\n",
    "    # 18、scoreKeyword2 \n",
    "    # 19、scoreAbstract1\n",
    "    # 10、scoreAbstract2\n",
    "    # 21、scoreAbstract3 \n",
    "    # 21、scoreAbstract4\n",
    "    \n",
    "    feature = [round(scoreTitle, 6),\n",
    "               round(scoreAuthor1, 6),\n",
    "               round(scoreAuthor2, 6),\n",
    "               round(scoreAuthor3, 6),\n",
    "               round(scoreAuthor4, 6),\n",
    "               round(scoreAuthor5, 6),\n",
    "               round(scoreAuthor6, 6),\n",
    "               round(scoreVenue1, 6),\n",
    "               round(scoreVenue2, 6),\n",
    "               round(scoreOrg1, 6),\n",
    "               round(scoreOrg2, 6),\n",
    "               round(scoreOrg3, 6),\n",
    "               round(scoreOrg4, 6),\n",
    "               round(scoreOrg5, 6),\n",
    "               round(scoreOrg6, 6),\n",
    "               round(scoreYear, 6),\n",
    "               round(scoreKeyword1, 6),\n",
    "               round(scoreKeyword2, 6),\n",
    "               round(scoreAbstract1, 6),\n",
    "               round(scoreAbstract2, 6),\n",
    "               round(scoreAbstract3, 6),\n",
    "               round(scoreAbstract4, 6)\n",
    "              ]\n",
    "    return feature \n",
    "\n",
    "authorData = defaultdict(list)\n",
    "for item in authorPaper:\n",
    "    name = cleanName(authorPaper[item][\"name\"])\n",
    "    authorData[name].append(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.1, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=10, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(\n",
    "                            n_estimators=1000,\n",
    "                            criterion='entropy',\n",
    "                            max_depth=9,\n",
    "                            bootstrap=True,\n",
    "                            random_state=0,\n",
    "                            warm_start=False,\n",
    "                            class_weight=None,\n",
    "                            n_jobs=10,\n",
    "                            min_impurity_decrease=0.1\n",
    "                            )\n",
    "rf.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuary: 92.22%\n"
     ]
    }
   ],
   "source": [
    "randomforest = rf.predict_proba(xTest)\n",
    "train_predictions6 = np.argmax(randomforest, 1)\n",
    "y_train = dtest.get_label() #值为输入数据的第一行\n",
    "train_accuracy = accuracy_score(y_train, train_predictions6)\n",
    "print (\"Train Accuary: %.2f%%\" % (train_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 22 artists>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(rf.feature_importances_)),(rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9650/9650 [37:56<00:00,  4.24it/s]  \n"
     ]
    }
   ],
   "source": [
    "result_dict = defaultdict(list)\n",
    "total = 0\n",
    "for item in tqdm(testData):\n",
    "    paperId, index = item.split('-')\n",
    "    paperInfo = testPublic[paperId]\n",
    "    name = paperInfo[\"authors\"]\n",
    "    name = (cleanName(name[int(index)][\"name\"]))\n",
    "    if name == 'l_deng':\n",
    "        name = 'li_deng'\n",
    "    if name == 'osamu':\n",
    "        name = 'osamu_watanabe'\n",
    "    if name == 'jun':\n",
    "        name = 'jun_yang'\n",
    "    if name == 'bo':\n",
    "        name = 'bo_li'\n",
    "    if name == 'b_liu':\n",
    "        name = 'bin_liu'\n",
    "    if name == 'lin':\n",
    "        name = 'lin_he'\n",
    "    if name == 'fei':\n",
    "        name = 'fei_wei'\n",
    "    if name == 'liu,_ling':\n",
    "        name = 'ling_liu'\n",
    "    if name == 'xie_d':\n",
    "        name = 'dan_xie'\n",
    "    if name == 'k_zhou':\n",
    "        name = 'kun_zhou'\n",
    "    if name == 'hui':\n",
    "        name = 'hui_zhang'\n",
    "    if name == 'hang':\n",
    "        name = 'hang_li'\n",
    "    if name == 'jianping':\n",
    "        name = 'jianping_ding'\n",
    "    if name == 'jinghong':\n",
    "        name = 'jinghong_li'\n",
    "    if name == 'jianfang':\n",
    "        name = 'jianfang_wang'\n",
    "    if name in authorData.keys():\n",
    "        candidate = authorData[name]\n",
    "    else:\n",
    "        temp = name.split('_')\n",
    "        if len(temp) == 2:\n",
    "            a = temp[1]\n",
    "            temp[1] = temp[0]\n",
    "            temp[0] = a\n",
    "        name = '_'.join(temp) \n",
    "        if name in authorData.keys():\n",
    "            candidate = authorData[name]\n",
    "        else:\n",
    "            candidate = []\n",
    "    \n",
    "    classifySet = []\n",
    "    for personId in candidate:\n",
    "        exam = paperId + '-' + personId \n",
    "        temp = (getFeature(exam) - mean)/std\n",
    "        classifySet.append(temp.tolist())\n",
    "    if classifySet == []:\n",
    "        continue\n",
    "    prob = rf.predict_proba((classifySet))[:, 1]\n",
    "    rank = np.argsort(-np.array(prob))\n",
    "\n",
    "    result_dict[candidate[rank[0]]].append((paperId, np.array(prob)[rank[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     3,
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "result = defaultdict(list)\n",
    "threshold = 0.076\n",
    "num = 0\n",
    "for items in result_dict:\n",
    "    for item in result_dict[items]:\n",
    "        paperId, prob = item\n",
    "        if prob >= threshold:\n",
    "            result[items].append(paperId)\n",
    "        else:\n",
    "            num += 1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open(\"./result.json\", 'w') as files:\n",
    "    json.dump(result, files, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
